name: Scrape and Analyze Articles with GPT

on:
  # Uruchamiaj codziennie o 14:30 UTC (16:30 czasu polskiego w zimie, 15:30 w lecie)
  schedule:
    - cron: '30 14 * * *'
  
  # PozwÃ³l na rÄ™czne uruchomienie
  workflow_dispatch:
  
  # Uruchamiaj na push do main/master (opcjonalne)
  push:
    branches: [ master, main ]
    paths:
      - 'clickbait_verifier/**'
      - 'config.yaml'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Configure Git
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions Bot"
    
    - name: Scrape articles
      id: scrape
      run: |
        python -m clickbait_verifier.main
        echo "scrape_completed=true" >> $GITHUB_OUTPUT
      continue-on-error: false
    
    - name: Commit scraped results
      if: steps.scrape.outcome == 'success'
      run: |
        git add reports/scraped/*.json
        
        if git diff --staged --quiet; then
          echo "No scraped files to commit"
        else
          git commit -m "ðŸ¤– Auto: scraped articles [$(date +'%Y-%m-%d %H:%M')]"
          git push
        fi
      continue-on-error: true

  analyze:
    runs-on: ubuntu-latest
    needs: scrape
    if: success()
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Configure Git
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions Bot"
    
    - name: Analyze with GPT
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python analyze_today.py
      continue-on-error: true
    
    - name: Commit analysis results
      run: |
        git add reports/analysis/*.json
        
        if git diff --staged --quiet; then
          echo "No analysis files to commit"
        else
          git commit -m "ðŸ¤– Auto: analyzed articles with GPT [$(date +'%Y-%m-%d %H:%M')]"
          git push
        fi
      continue-on-error: true
    
    - name: Summary
      if: always()
      run: |
        echo "## ðŸ“Š Analysis Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Total analysis files:** $(ls reports/analysis/ 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- **Date:** $(date)" >> $GITHUB_STEP_SUMMARY
    
    - name: Commit and push results
      run: |
        git add reports/scraped/*.json
        git add reports/analysis/*.json
        
        # SprawdÅº czy sÄ… zmiany
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "ðŸ¤– Auto: scraped and analyzed articles [$(date +'%Y-%m-%d %H:%M')]"
          git push
        fi
      continue-on-error: true
    
    - name: Summary
      if: always()
      run: |
        echo "## ðŸ“Š Workflow Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Scraped files:** $(ls reports/scraped/scraped_$(date +%s | cut -c1-10)*.json 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- **Analysis files:** $(ls reports/analysis/ | wc -l)" >> $GITHUB_STEP_SUMMARY
        echo "- **Date:** $(date)" >> $GITHUB_STEP_SUMMARY